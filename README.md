# Awesome GNN2MLP
Learning MLPs to replace GNN


[2023-04-17] This is our first release, feel free to contact us if you find any missing important Papers. You are also welcome to create a pull request to contribute anything you feel useful!
[2023-06-03] Add one paper

---
# Papers
## Knowledge Distillation

- (ICLR'22) Graph-Less Neural Networks: Teaching Old MLPs New Tricks Via Distillation. | [Paper](https://arxiv.org/abs/2110.08727) | [Code](https://github.com/snap-research/graphless-neural-networks)

- (ICLR'23) NOSMOG: Learning Noise-robust and Structure-aware MLPs on Graphs. | [Paper](https://arxiv.org/pdf/2208.10010v1.pdf) | [Code](https://github.com/meettyj/NOSMOG)

- SA-MLP: Distilling Graph Knowledge from GNNs into Structure-Aware MLP. | [Paper](https://arxiv.org/pdf/2210.09609)

- Linkless Link Prediction via Relational Distillation | [Paper](https://openreview.net/forum?id=He7UIpiEq_O
)

-  Extracting Low-/High- Frequency Knowledge from Graph Neural Networks and Injecting it into MLPs: An Effective GNN-to-MLP Distillation Framework | [Paper](https://arxiv.org/abs/2305.10758) | [Code](https://github.com/LirongWu/FF-G2M)

## Regularization

- Graph-MLP: Node Classification without Message Passing in Graph. | [Paper](https://arxiv.org/pdf/2106.04051v1.pdf)

- OrthoReg: Improving Graph-regularized MLPs via Orthogonality Regularization. | [Paper](https://openreview.net/forum?id=5s2v_0F7MG)

## Knowledge Distillation & Regularization

- TEACHING YOURSELF: GRAPH SELF-DISTILLATION ON NEIGHBORHOOD FOR NODE CLASSIFICATION. | [Paper](https://arxiv.org/pdf/2210.02097v4.pdf)

## Knowledge Distillation wiz Graph Edges

- Edge-free but Structure-aware: Prototype-Guided Knowledge Distillation from GNNs to MLPs. | [Paper](https://arxiv.org/abs/2303.13763)


# Other Resources
- https://github.com/ZhuYun97/awesome-GNN-distillation

